\documentclass{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\newtheorem{theorem}{Theorem}
\title{Mathematical Content Collection}
\author{Compiled from Stack Exchange, ProofWiki, and ArXiv}
\date{\today}
\begin{document}
\maketitle
\tableofcontents
\newpage

\section{(A cap C) cup (B cap Complement C) = Empty iff B subset C subset Complement A}
\textit{Source: Proofwiki}

\begin{theorem}
Let $A$, $B$ and $C$ besubsetsof auniverse$\Bbb U$. where $\map \complement C$ denotes thecomplementof $C$ in $\Bbb U$.
\end{theorem}

\begin{proof}
Proof omitted.
\end{proof}

\newpage

\section{*-Algebra Homomorphism between C*-Algebras is Norm-Decreasing}
\textit{Source: Proofwiki}

\begin{theorem}
Let $\struct {A, \ast, \norm {\, \cdot \,}_A}$ and $\struct {B, \dagger, \norm {\, \cdot \,}_B}$ be$\text C^\ast$-algebras. Let $\phi : A \to B$ be a$\ast$-algebra homomorphism. FromSpectrum of Image of Element of Unital Algebra under Unital Algebra Homomorphism: Corollary, we have:
\end{theorem}

\begin{proof}
Let $x \in A$. FromSpectrum of Image of Element of Unital Algebra under Unital Algebra Homomorphism: Corollary, we have: Hence we have: Hence we obtain: $\blacksquare$
\end{proof}

\newpage

\section{*-Algebra Homomorphism preserves Canonical Preordering of C*-Algebra}
\textit{Source: Proofwiki}

\begin{theorem}
Let $\struct {A, \ast, \norm {\, \cdot \,}_A}$ and $\struct {B, \square, \norm {\, \cdot \,}_B}$ be$\text C^\ast$-algebras. Let $\phi : A \to B$ be a$\ast$-algebra homomorphism. Let $\le_A$ and $\le_B$ be thecanonical preorderingson $A$ and $B$ respectively.
\end{theorem}

\begin{proof}
Since $\phi$ is analgebra homomorphism, we aim to show that: Since $y - x$ and $z - y$ areHermitianand $\phi$ is a$\ast$-algebra homomorphism, we have that: Since $y - x$ are $z - y$ arepositive, we have: Hence fromSpectrum of Image of Element of Unital Algebra under Unital Algebra Homomorphism: Corollary, we have: Hence $\map \phi {y - x}$ and $\map \phi {z - y}$ arepositive. Hence $\map \phi y - \map \phi x$ and $\map \phi z - \map \phi y$ arepositive. $\blacksquare$
\end{proof}

\newpage

\section{*-Algebra obtains Banach *-Algebra Structure through *-Algebra Isomorphism}
\textit{Source: Proofwiki}

\begin{theorem}
Let $\struct {A, \ast}$ be a$\ast$-algebraover $\C$. Let $\struct {B, \square, \norm {\, \cdot \,}_B}$ be aBanach $\ast$-algebra. Let $\phi : A \to B$ be a$\ast$-algebra isomorphism.
\end{theorem}

\begin{proof}
FromAlgebra obtains Norm Structure through Algebra Isomorphism: We want to show that: for each $a \in A$. Let $a \in A$. By definition, $\phi : A \to B$ is anisometric isomorphism. Hence fromInverse of Isometric Isomorphism between Normed Vector Spaces is Isometric Isomorphism, $\phi^{-1} : B \to A$ is anisometric isomorphism. FromMetric Space Completeness is Preserved by Isometry, thecompletenessof $B$ implies thecompletenessof $A$. So $\struct {A, \norm {\, \cdot \,}_A}$ is aBanach algebra. Hence $\struct {A, \ast, \norm {\, \cdot \,}_A}$ is aBanach $\ast$-algebra. Now suppose that $\struct {B, \square, \norm {\, \cdot \,}_B}$ is a$\ast$-algebraso that: for each $b \in B$. We want to show that: for each $a \in A$. $\blacksquare$
\end{proof}

\newpage

\section{-1\textbackslash{}^{}n by -n choose k-1 equals -1\textbackslash{}^{}k by -k choose n-1}
\textit{Source: Proofwiki}

\begin{theorem}
Let $n, k \in \Z_{\ge 0}$.
\end{theorem}

\newpage

\section{0}
\textit{Source: Proofwiki}

\begin{theorem}
The Babylonians from the $2$nd century BCE used anumber basesystem of arithmetic, with a placeholder to indicate that a particular place within a number was empty, but its use was inconsistent.  However, they had no actual recognition ofzeroas a mathematical concept in its own right. The Ancient Greeks had no conception ofzeroas a number. The concept ofzerowas invented by the mathematicians of India. TheBakhshali Manuscriptfrom the $3$rd century CE contains the first reference to it.
\end{theorem}

\newpage

\section{0.999...=1}
\textit{Source: Proofwiki}

\begin{theorem}
BySum of Infinite Geometric Sequence: where $a = \dfrac 9 {10}$ and $r = \dfrac 1 {10}$. Since our ratio is less than $1$, then we know that $\ds \sum_{n \mathop = 0}^\infty \frac 9 {10} \paren {\frac 1 {10} }^n$ must converge to:
\end{theorem}

\begin{proof}
BySum of Infinite Geometric Sequence: where $a = \dfrac 9 {10}$ and $r = \dfrac 1 {10}$. Since our ratio is less than $1$, then we know that $\ds \sum_{n \mathop = 0}^\infty \frac 9 {10} \paren {\frac 1 {10} }^n$ must converge to: $\blacksquare$
\end{proof}

\newpage

\section{0.999...=1/Proof 1}
\textit{Source: Proofwiki}

\begin{theorem}
BySum of Infinite Geometric Sequence: where $a = \dfrac 9 {10}$ and $r = \dfrac 1 {10}$. Since our ratio is less than $1$, then we know that $\ds \sum_{n \mathop = 0}^\infty \frac 9 {10} \paren {\frac 1 {10} }^n$ must converge to:
\end{theorem}

\begin{proof}
BySum of Infinite Geometric Sequence: where $a = \dfrac 9 {10}$ and $r = \dfrac 1 {10}$. Since our ratio is less than $1$, then we know that $\ds \sum_{n \mathop = 0}^\infty \frac 9 {10} \paren {\frac 1 {10} }^n$ must converge to: $\blacksquare$
\end{proof}

\newpage

\section{0.999...=1/Proof 2}
\textit{Source: Proofwiki}

\begin{proof}
Proof omitted.
\end{proof}

\newpage

\section{0.999...=1/Proof 3}
\textit{Source: Proofwiki}

\begin{theorem}
Let  $c = 0.999 \ldots$
\end{theorem}

\begin{proof}
Let  $c = 0.999 \ldots$ It follows that: $\blacksquare$
\end{proof}

\newpage

\section{0.999...=1/Proof 4}
\textit{Source: Proofwiki}

\begin{theorem}
We begin with the knowledge that: Now we divide $9$ by $9$ using the standard process of long division, only instead of stating that $90$ divided by $9$ is $10$, we say that it is "$9$ remainder $9$," yielding the following result: Thus, we are compelled to believe that:
\end{theorem}

\begin{proof}
We begin with the knowledge that: Now we divide $9$ by $9$ using the standard process of long division, only instead of stating that $90$ divided by $9$ is $10$, we say that it is "$9$ remainder $9$," yielding the following result: Thus, we are compelled to believe that: $\blacksquare$
\end{proof}

\newpage

\section{0.999...=1/Proof 5}
\textit{Source: Proofwiki}

\begin{proof}
Proof omitted.
\end{proof}

\newpage

\section{0 in B-Algebra is Left Cancellable Element}
\textit{Source: Proofwiki}

\begin{theorem}
Let $\struct {X, \circ}$ be a$B$-Algebra. Let $x, y \in X$ and let $0 \circ x = 0 \circ y$. From$B$-Algebra Identity: $x \circ y = 0 \iff x = y$:
\end{theorem}

\begin{proof}
Let $x, y \in X$ and let $0 \circ x = 0 \circ y$. So we have shown: From$B$-Algebra Identity: $x \circ y = 0 \iff x = y$: Hence the result. $\blacksquare$
\end{proof}

\newpage

\section{1}
\textit{Source: Proofwiki}

\begin{theorem}
The ancient Greeks did not consider $1$ to be a number. According to thePythagoreans, the numberOne ($1$)was the Generator of all Numbers: the omnipotent One. It representedreason, forreasoncould generate only $1$ self-evident body of truth.
\end{theorem}

\newpage

\section{1+1 = 2}
\textit{Source: Proofwiki}

\begin{theorem}
Define $0$ as theuniqueelementin theset$P \setminus \map s P$, where: $1$ is definedby hypothesisas $\map s 0$ and $2$ as $\map s {\map s 0}$. Hence the statement to be proven becomes:
\end{theorem}

\begin{proof}
$1$ is definedby hypothesisas $\map s 0$ and $2$ as $\map s {\map s 0}$. Hence the statement to be proven becomes: $\blacksquare$
\end{proof}

\newpage

\section{1+2+...+n+(n-1)+...+1 = n\textbackslash{}^{}2}
\textit{Source: Proofwiki}

\begin{proof}
Proof omitted.
\end{proof}

\newpage

\section{Can every proof by contradiction also be shown without contradiction?}
\textit{Source: Stackexchange}

\subsection*{Question}
Are there some proofs that can only be shown by contradiction or can everything that can be shown by contradiction also be shown without contradiction? What are the advantages/disadvantages of proving by contradiction? As an aside, how is proving by contradiction viewed in general by 'advanced' mathematicians. Is it a bit of an 'easy way out' when it comes to trying to show something or is it perfectly fine? I ask because one of our tutors said something to that effect and said that he isn't fond of proof by contradiction.

\subsection*{Answer}
To determine what can and cannot be proved by contradiction, we have to formalize a notion of proof. As a piece of notation, we let $\bot$ represent an identically false proposition. Then $\lnot A$, the negation of $A$, is equivalent to $A \to \bot$, and we take the latter to be the definition of the former in terms of $\bot$. There are two key logical principles that express different parts of what we call "proof by contradiction": The principle of explosion: for any statement $A$, we can take "$\bot$ implies $A$" as an axiom. This is also called ex falso quodlibet. The law of the excluded middle: for any statement $A$, we can take "$A$ or $\lnot A$" as an axiom. In proof theory, there are three well known systems: Minimal logic has neither of the two principles above, but it has basic proof rules for manipulating logical connectives (other than negation) and quantifiers. This system corresponds most closely to "direct proof", because it does not let us leverage a negation for any purpose. Intuitionistic logic includes minimal logic and the principle of explosion Classical logic includes intuitionistic logic and the law of the excluded middle It is known that there are statements that are provable in intuitionistic logic but not in minimal logic, and there are statements that are provable in classical logic that are not provable in intuitionistic logic. In this sense, the principle of explosion allows us to prove things that would not be provable without it, and the law of the excluded middle allows us to prove things we could not prove even with the principle of explosion. So there are statements that are provable by contradiction that are not provable directly. The scheme "If $A$ implies a contradiction, then $\lnot A$ must hold" is true even in intuitionistic logic, because $\lnot A$ is just an abbreviation for $A \to \bot$, and so that scheme just says "if $A \to \bot$ then $A \to \bot$". But in intuitionistic logic, if we prove $\lnot A \to \bot$, this only shows that $\lnot \lnot A$ holds. The extra strength in classical logic is that the law of the excluded middle shows that $\lnot \lnot A$ implies $A$, which means that in classical logic if we can prove $\lnot A$ implies a contradiction then we know that $A$ holds. In other words: even in intuitionistic logic, if a statement implies a contradiction then the negation of the statement is true, but in classical logic we also have that if the negation of a statement implies a contradiction then the original statement is true, and the latter is not provable in intuitionistic logic, and in particular is not provable directly.

\newpage

\section{Why are mathematical proofs that rely on computers controversial?}
\textit{Source: Stackexchange}

\subsection*{Question}
There are many theorems in mathematics that have been proved with the assistance of computers, take the famous four color theorem for example. Such proofs are often controversial among some mathematicians. Why is it so? I my opinion, shifting from manual proofs to computer-assisted proofs is a giant leap forward for mathematics. Other fields of science rely on it heavily. Physics experiments are simulated in computers. Chemical reactions are simulated in supercomputers. Even evolution can be simulated in an advanced enough computer. All of this can help us understand these phenomena better. But why are mathematicians so reluctant?

\subsection*{Answer}
What is mathematics? One answer is that mathematics is a collection of definitions, theorems, and proofs of them. But the more realistic answer is that mathematics is what mathematicians do. (And partly, that's a social activity.) Progress in mathematics consists of advancing human understanding of mathematics. What is a proof for? Often we pretend that the reason for a proof is so that we can be sure that the result is true. But actually what mathematicians are looking for is understanding. I encourage everyone to read the article On Proof and Progress in Mathematics by the Fields Medalist William Thurston. He says (on page 2): The rapid advance of computers has helped dramatize this point, because computers and people are very different. For instance, when Appel and Haken completed a proof of the 4-color map theorem using a massive automatic computation, it evoked much controversy. I interpret the controversy as having little to do with doubt people had as to the veracity of the theorem or the correctness of the proof. Rather, it reflected a continuing desire for human understanding of a proof, in addition to knowledge that the theorem is true. On a more everyday level, it is common for people first starting to grapple with computers to make large-scale computations of things they might have done on a smaller scale by hand. They might print out a table of the first 10,000 primes, only to find that their printout isn’t something they really wanted after all. They discover by this kind of experience that what they really want is usually not some collection of “answers”—what they want is understanding. Some people may claim that there is doubt about a proof when it has been proved by a computer, but I think human proofs have more room for error. The real issue is that (long) computer proofs (as opposed to, something simple like checking a numerical value by calculator) are hard to keep in your head. Compare these quotes from Gian-Carlo Rota's Indiscrete Thoughts, where he describes the mathematicians' quest for understanding: “eventually every mathematical problem is proved trivial. The quest for ultimate triviality is characteristic of the mathematical enterprise.” (p.93) “Every mathematical theorem is eventually proved trivial. The mathematician’s ideal of truth is triviality, and the community of mathematicians will not cease its beaver-like work on a newly discovered result until it has shown to everyone’s satisfaction that all difficulties in the early proofs were spurious, and only an analytic triviality is to be found at the end of the road.” (p. 118, in The Phenomenology of Mathematical Truth) Are there definitive proofs? It is an article of faith among mathematicians that after a new theorem is discovered, other simpler proofs of it will be given until a definitive one is found. A cursory inspection of the history of mathematics seems to confirm the mathematician’s faith. The first proof of a great many theorems is needlessly complicated. “Nobody blames a mathematician if the first proof of a new theorem is clumsy”, said Paul Erdős. It takes a long time, from a few decades to centuries, before the facts that are hidden in the first proof are understood, as mathematicians informally say. This gradual bringing out of the significance of a new discovery takes the appearance of a succession of proofs, each one simpler than the preceding. New and simpler versions of a theorem will stop appearing when the facts are finally understood. (p.146, in The Phenomenology of Mathematical Proof). In my opinion, there is nothing wrong with, or doubtful about, a proof that relies on computer. However, such a proof is in the intermediate stage described above, that has not yet been rendered trivial enough to be held in a mathematician's head, and thus the theorem being proved is to be considered still work in progress.

\newpage

\section{Does notation ever become \textbackslash{}&quot;easier\textbackslash{}&quot;?}
\textit{Source: Stackexchange}

\subsection*{Question}
I'm in my first semester of college going for a math major and it's pretty great. I'm doing well, however, there seems to be huge gap between how difficult /complex an idea is and how convoluted it is presented. Let me make an example: In Analysis we discussed the Bolzano Weierstrass theorem and one of the lemmas showed that every sequence in $\mathbb{R}$ has a monotone subsequence. The idea behind the proof with the maximum spots ( speaking colloquially here ) is super simple and pretty elegant if you asked me, but I spent a significant amount of time trying to understand the notation of the professor until I went to this site to read a "proper explanation" of the proof, which had much simpler notation in it. Extracting the idea of the proof took me lots of time because of the strange notation, but once you understand what is going on, it is really easy. Most of the time spent studying lectures is about digging through the formalities. Do I just have to spend more time really going through all the formal details of a proof to become accustomed to that formality? Or do more advanced mathematicians also struggle to extract the ideas from the notation? I'd assume there will come a point, where the idea itself is the most complex part, so I do not want to get stuck at the notation, when that happens. ( Proof Verification - Every sequence in $\Bbb R$ contains a monotone sub-sequence if you are interested )

\subsection*{Answer}
As others have pointed out, it gets much better if that's your first semester. But in my experience, there is not much relief between, say, years 2 and 4 of your studies. Sure, you get more mature, but the material gets more difficult too. To address your question whether "more advanced mathematicians also struggle to extract the ideas from the notation", I'd like to quote V.I. Arnold, since I think it's exactly in the spirit of your frustration. It is almost impossible for me to read contemporary mathematicians who, instead of saying "Petya washed his hands," write simply: "There is a $t_1<0$ such that the image of $t_1$ under the natural mapping $t_1 \mapsto {\rm Petya}(t_1)$ belongs to the set of dirty hands, and a $t_2$, $t_1<t_2 \leq 0$, such that the image of $t_2$ under the above-mentioned mapping belongs to the complement of the set defined in the preceding sentence.'' The trade-off is clear: without rigor math would've been quite a mess. But if rigor is the only way math gets communicated to someone, this person simply won't have time to get far in math.

\newpage

\section{Use of \textbackslash{}&quot;without loss of generality\textbackslash{}&quot;}
\textit{Source: Stackexchange}

\subsection*{Question}
Why do we use "without loss of generality" when writing proofs? Is it necessary or convention? What "synonym" can be used?

\subsection*{Answer}
I think this is great question, as the mathematical use of "without loss of generality" often varies from its literal meaning. The literal meaning is when you rephrase a general statement $P(x)$ is true for all $x \in S$, using another set (which is easier to work with) $P(z)$ is true for all $z \in T$, where $P$ is some property of elements in $S$ and $T$, and it can be shown (or is known) that $S=T$. For example: We want to show that $P(x)$ is true for all $x \in \mathbb{Z}$. Without loss of generality, we can assume that $x=z+1$ for some $z \in \mathbb{Z}$. [In this case, $S=\mathbb{Z}$ and $T=\{z+1:z \in \mathbb{Z}\}$.] We want to show that $P(x)$ is true for all $x \in \mathbb{Z}$. Without loss of generality, we can assume that $x=5q+r$ where $q,r \in \mathbb{Z}$ and $0 \leq r < q$. [In this case, $S=\mathbb{Z}$ and $T=\{5q+r:q \in \mathbb{Z} \text{ and } r \in \mathbb{Z} \text{ and } 0 \leq r < q\}$.] In the above instances, indeed no generality has been lost, since in each case we can prove $S=T$ (or, more likely, it would be assumed that the reader can deduce that $S=T$). I.e., proving that $P(z)$ holds for $z \in T$ is the same as proving that $P(x)$ holds for $x \in S$. The above cases are examples of clear-cut legitimate usage of "without loss of generality", but there is a widespread second use. Wikipedia writes: The term is used before an assumption in a proof which narrows the premise to some special case; it is implied that the proof for that case can be easily applied to all others (or that all other cases are equivalent). Thus, given a proof of the conclusion in the special case, it is trivial to adapt it to prove the conclusion in all other cases. [emphasis mine.] So, paradoxically, "without loss of generality" is often used to highlight when the author has deliberately lost generality in order to simplify the proof. Thus, we are rephrasing a general statement: $P(x)$ is true for all $x \in S$, as $P(z)$ is true for all $z \in T$, and if $x \in S$, then there exists $z \in T$ for which $P(x)$ is true if $P(z)$ is true. For example: Let $S$ be a set of groups of order $n$. We want to show $P(G)$ is true for all $G \in S$. Without loss of generality, assume the underlying set of $G$ is $\{0,1,\ldots n-1\}$ for some $n \geq 1$. [Here, $T$ is a set of groups with underlying set $\{0,1,\ldots n-1\}$ that are isomorphic to groups in $S$, and the reader is assumed to be able to deduce that property $P$ is preserved by isomorphism.] My personal preference is to replace the second case with: "It is sufficient to prove $P(z)$ for $z \in T$, since [[for some reason]] it follows that $P(x)$ is true for all $x \in S$."

\newpage

\section{How do I prove that a function is well defined?}
\textit{Source: Stackexchange}

\subsection*{Question}
How do you in general prove that a function is well-defined? $$f:X\to Y:x\mapsto f(x)$$ I learned that I need to prove that every point has exactly one image. Does that mean that I need to prove the following two things: Every element in the domain maps to an element in the codomain: $$x\in X \implies f(x)\in Y$$ The same element in the domain maps to the same element in the codomain: $$x=y\implies f(x)=f(y)$$ At the moment I'm trying to prove this function is well-defined: $$f:(\Bbb Z/12\mathbb Z)^∗→(\Bbb Z/4\Bbb Z)^∗:[x]_{12}↦[x]_4 ,$$ but I'm more interested in the general procedure.

\subsection*{Answer}
When we write $f\colon X\to Y$ we say three things: $f\subseteq X\times Y$. The domain of $f$ is $X$. Whenever $\langle x,y_1\rangle,\langle x,y_2\rangle\in f$ then $y_1=y_2$. In this case whenever $\langle x,y\rangle\in f$ we denote $y$ by $f(x)$. So to say that something is well-defined is to say that all three things are true. If we know some of these we only need to verify the rest, for example if we know that $f$ has the third property (so it is a function) we need to verify its domain is $X$ and the range is a subset of $Y$. If we know those things we need to verify the third condition. But, and that's important, if we do not know that $f$ satisfies the third condition we cannot write $f(x)$ because that term assumes that there is a unique definition for that element of $Y$.

\newpage

\section{Prove: If a sequence converges, then every subsequence converges to the same limit.}
\textit{Source: Stackexchange}

\subsection*{Question}
I need some help understanding this proof: Prove: If a sequence converges, then every subsequence converges to the same limit. Proof: Let $s_{n_k}$ denote a subsequence of $s_n$. Note that $n_k \geq k$ for all $k$. This easy to prove by induction: in fact, $n_1 \geq 1$ and $n_k \geq k$ implies $n_{k+1} > n_k \geq k$ and hence $n_{k+1} \geq k+1$. Let $\lim s_n = s$ and let $\epsilon > 0$. There exists $N$ so that $n>N$ implies $|s_n - s|  N \implies n_k > N \implies |s_{n_k} - s| < \epsilon$. Therefore: $\lim_{k \to \infty} s_{n_k} = s$. What is the intuition that each subsequence will converge to the same limit I do not understand the induction that claims $n_k \geq k$

\subsection*{Answer}
A sequence converges to a limit $L$ provided that, eventually, the entire tail of the sequence is very close to $L$. If you restrict your view to a subset of that tail, it will also be very close to $L$. An example might help. Suppose your subsequence is to take every other index: $n_1 = 2$, $n_2 = 4$, etc. In general, $n_k = 2k$. Notice $n_k \geq k$, since each step forward in the sequence makes $n_k$ increase by $2$, but $k$ increases only by $1$. The same will be true for other kinds of subsequences (i.e. $n_k$ increases by at least $1$, while $k$ increases by exactly $1$).

\newpage

\section{Strategies to denest nested radicals $\sqrt{a+b\sqrt{c}}$}
\textit{Source: Stackexchange}

\subsection*{Question}
I have recently read some passage about nested radicals, I'm deeply impressed by them. Simple nested radicals $\sqrt{2+\sqrt{2}}$,$\sqrt{3-2\sqrt{2}}$ which the later can be denested into $1-\sqrt{2}$. This may be able to see through easily, but how can we denest such a complicated one $\sqrt{61-24\sqrt{5}}(=4-3\sqrt{5})$? And Is there any ways to judge if a radical in $\sqrt{a+b\sqrt{c}}$ form can be denested? Mr. Srinivasa Ramanujan even suggested some CRAZY nested radicals such as: $$\sqrt[3]{\sqrt{2}-1},\sqrt{\sqrt[3]{28}-\sqrt[3]{27}},\sqrt{\sqrt[3]{5}-\sqrt[3]{4}}, \sqrt[3]{\cos{\frac{2\pi}{7}}}+\sqrt[3]{\cos{\frac{4\pi}{7}}}+\sqrt[3]{\cos{\frac{8\pi}{7}}},\sqrt[6]{7\sqrt[3]{20}-19},...$$ Amazing, these all can be denested. I believe there must be some strategies to denest them, but I don't know how. I'm a just a beginner, can anyone give me some ideas? Thank you.

\subsection*{Answer}
There do exist general denesting algorithms employing Galois theory, but for the simple case of quadratic algebraic numbers we can employ a simple rule that I discovered as a teenager. $$\bbox[1px,border:1px solid #0a0]{\bbox[8px,border:1px solid #0a0]{\rm {\bf Simple\ Denesting\ Rule}\!:\ \ \color{blue}{subtract\ out}\ \sqrt{norm},\, \ then\ \ \color{brown}{divide\ out}\ \sqrt{trace}\ }}\qquad\ \ $$ Recall $\rm\: w = a + b\sqrt{n}\: $ has norm $\rm =\: w\:\cdot\: w' = (a + b\sqrt{n})\ \cdot\: (a - b\sqrt{n})\ =\: a^2 - n\, b^2 $ and, $ $ furthermore, $\rm\ w^{\phantom{|^|}}$ has $ $ trace $\rm\: = w+w' = (a + b\sqrt{n}) + (a - b\sqrt{n})\: =\: 2a$ Here $\:61-24\sqrt{5}\:$ has norm $= 29^2.\:$ $\rm \color{blue}{Subtracting\ out}\ \sqrt{norm}\ = 29\ $ yields $\ \color{#0a0}{32\:\!-2\:\!4\sqrt{5}}\:$ and $\rm\color{#0a0}{this}$ has $\rm\ \sqrt{trace}\: =\: 8,\ \ thus, \ \ \color{brown}{dividing \ it \ out}\, $ of $\rm\color{#0a0}{this}$ yields the sqrt: $\,\pm( 4\,-\,3\sqrt{5}).$ See here for a simple proof of the rule, and see here for many examples of its use.

\newpage

\section{Will assuming the existence of a solution ever lead to a contradiction?}
\textit{Source: Stackexchange}

\subsection*{Question}
I'm reading Manfredo Do Carmo's differential geometry book. In section 1-7, he discusses the "Isoperimetric Inequality" which is related to the question of what 2-dimensional shape maximizes the enclosed area for a closed curve of constant length. He mentions that A satisfactory proof of the fact that the circle is a solution to the isoperimetric problem took, however, a long time to appear. The main reason seems to be that the earliest proofs assumed that a solution should exist. It was only in 1870 that K. Weierstrass pointed out that many similar questions did not have solutions. This line of reasoning would suggest that assuming the existence of a solution might lead to a contradiction (such as an apparent solution that is not in fact valid). Is this actually a problem? Are there any problems that produce invalid solutions under the (flawed) assumption that a solution exists at all? If so, what is an example and how does it differ from the statement of the isoperimetric problem?

\subsection*{Answer}
Just the first thing that came to my mind... assume $A=\sum_{n=0}^{\infty}2^n $ exists, it is very easy to find $A $: note $A=1+2\sum_{n=0}^{\infty}2^n =1+2A $, so $A=-1$. Of course, this is all wrong precisely because $A $ does not exist.

\newpage

\section{Could I be using proof by contradiction too much?}
\textit{Source: Stackexchange}

\subsection*{Question}
Lately, I've developed a habit of proving almost everything by contradiction. Even for theorems for which direct proofs are the clear choice, I'd just start by writing "Assume not" then prove it directly, thereby reaching a "contradiction." Is this a bad habit? I don't know why, but there's something incredibly satisfying about proof by contradiction.

\subsection*{Answer}
One general reason to avoid proof by contradiction is the following. When you prove something by contradiction, all you learn is that the statement you wanted to prove is true. When you prove something directly, you learn every intermediate implication you had to prove along the way. More explicitly, if you want to prove that $p \Rightarrow q$ by contradiction, you assume $p$ and $\neg q$ and derive a contradiction. None of the intermediate implications along the way can be reused because your premises were contradictory. If you want to prove that $p \Rightarrow q$ directly, say by proving that $p \Rightarrow p_1$ and $p_1 \Rightarrow p_2$ and so on until $p_n \Rightarrow q$, then you've also proven that $p_i \Rightarrow p_{i+1}$ for all of the relevant $i$. Many of these statements might be more useful than the original statement you were trying to prove. Another general reason to avoid a proof by contradiction is that it is often not explicit. For example, if you want to prove that something exists by contradiction, you can show that the assumption that it doesn't exist leads to a contradiction. But this doesn't necessarily give you a method for constructing the actual thing, which you might learn more from trying to do. A third reason is that frequently, or so it seems to me, a proof by contradiction is really a proof by contrapositive, where you assume $\neg q$ and derive $\neg p$. This feels like a proof by contradiction except that you never make use of the hypothesis $p$ except at the very end, and pretending that these are proofs by contradiction will make you blind to the fact that any intermediate implications you prove in a proof by contrapositive are still valid.

\newpage

\section{how to be good at proving?}
\textit{Source: Stackexchange}

\subsection*{Question}
I'm starting my Discrete Math class, and I was taught proving techniques such as proof by contradiction, contrapositive proof, proof by construction, direct proof, equivalence proof etc. I know how the proving system works and I can understand the sample proofs in my text to a sufficient extent. However, whenever I tried proving on my own, I got stuck, with no advancement of ideas in my head. How do you remedy this solution? Should i practise proving as much as possible? So far I've been googling proofs for my homework questions. But the final exam got proving questions (closed-book) so I need to come up with the proofs myself. We mainly focus on proving questions related to number theory. Should I read up on number theory and get acquainted with the properties of integers? I don't know how I should go about becoming proficient in proving. Can you guys share your experience on overcoming such an obstacle? What kind of resources do you use for this purpose? Thank you!

\subsection*{Answer}
I do not consider myself "good" at proving things. However, I know that I have gotten better. The key to writing a proof is understanding what you are trying to prove, which is harder than it may seem. Know your definitions. Often, I have been hampered or seen students hampered by not really knowing all of the definitions in the problem statement. Work with others. Look at what someone else has done in a proof and ask questions. Ask how they came up with the idea, ask that person to explain the proof to you. Also, do the same for them. Explain your proofs to a classmate and have them ask you questions. Try everything. Students often get stuck on proofs because they try one idea that does not work and give up. I often go through several bad ideas before getting anywhere on a proof. Another good strategy is to work with specific examples until you understand the problem. Plug in numbers and see why the theorem seems to be true. Also, try to construct a counterexample. The reason counterexamples fail often leads to a way to prove the statement.

\newpage

\section{What exactly is the difference between weak and strong induction?}
\textit{Source: Stackexchange}

\subsection*{Question}
I am having trouble seeing the difference between weak and strong induction. There are a few examples in which we can see the difference, such as reaching the $k^{th}$ rung of a ladder and proving every integer $>1$ can be written as a product of primes: To show every $n\ge2$ can be written as a product of primes, first we note that $2$ is prime. Now we assume true for all integers $2 \le m<n$. If $n$ is prime, we're done. If $n$ is not prime, then it is composite and so $n=ab$, where $a$ and $b$ are less than $n$. Since $a$ and $b$ are less than $n$, $ab$ can be written as a product of primes and hence $n$ can be written as a product of primes. QED However, it seems sort of like weak induction, only a bit dubious. In weak induction, we show a base case is true, then we assume true for all integers $k-1$, (or $k$), then we attempt to show it is true for $k$, (or $k+1$), which implies true $\forall n \in \mathbb N$. When we assume true for all integers $k$, isn't that the same as a strong induction hypothesis? That is, we're assuming true for all integers up to some specific one. As a simple demonstrative example, how would we show $1+2+\cdots+n= {n(n+1) \over 2}$ using strong induction? (Learned from Discrete Mathematics by Kenneth Rosen)

\subsection*{Answer}
Initial remarks: Good question. I think it deserves a full response (warning: this is going to be a long, but hopefully very clear, answer). First, most students do not really understand why mathematical induction is a valid proof technique. That's part of the problem. Second, weak induction and strong induction are actually logically equivalent; thus, differentiating between these forms of induction may seem a little bit difficult at first. The important thing to do is to understand how weak and strong induction are stated and to clearly understand the differences therein (I disagree with the previous answer that it is "just a matter of semantics"; it's not, and I will explain why). Much of what I will have to say is adapted from David Gunderson's wonderful book Handbook of Mathematical Induction, but I have expanded and tweaked a few things where I saw fit. That being said, hopefully you will find the rest of this answer to be informative. Gunderson remark about strong induction: While attempting an inductive proof, in the inductive step one often needs only the truth of $S(n)$ to prove $S(n+1)$; sometimes a little more "power" is needed (such as in the proof that any positive integer $n\geq 2$ is a product of primes--we'll explore why more power is needed in a moment), and often this is made possible by strengthening the inductive hypothesis. Kenneth Rosen remark in Discrete Mathematics and Its Applications Study Guide: Understanding and constructing proofs by mathematical induction are extremely difficult tasks for most students. Do not be discouraged, and do not give up, because, without doubt, this proof technique is the most important one there is in mathematics and computer science. Pay careful attention to the conventions to be observed in writing down a proof by induction. As with all proofs, remember that a proof by mathematical induction is like an essay--it must have a beginning, a middle, and an end; it must consist of complete sentences, logically and aesthetically arranged; and it must convince the reader. Be sure that your basis step (also called the "base case") is correct (that you have verified the proposition in question for the smallest value or values of $n$), and be sure that your inductive step is correct and complete (that you have derived the proposition for $k+1$, assuming the inductive hypothesis that proposition is true for $k$--or the slightly strong hypothesis that it is true for all values less than or equal to $k$, when using strong induction. Statement of weak induction: Let $S(n)$ denote a statement regarding an integer $n$, and let $k\in\mathbb{Z}$ be fixed. If (i) $S(k)$ holds, and (ii) for every $m\geq k, S(m)\to S(m+1)$, then for every $n\geq k$, the statement $S(n)$ holds. Statement of strong induction: Let $S(n)$ denote a statement regarding an integer $n$. If (i) $S(k)$ is true and (ii) for every $m\geq k, [S(k)\land S(k+1)\land\cdots\land S(m)]\to S(m+1)$, then for every $n\geq k$, the statement $S(n)$ is true. Proof of strong induction from weak: Assume that for some $k$, the statement $S(k)$ is true and for every $m\geq k, [S(k)\land S(k+1)\land\cdot\land S(m)]\to S(m+1)$. Let $B$ be the set of all $n>m$ for which $S(n)$ is false. If $B\neq\varnothing, B\subset\mathbb{N}$ and so by well-ordering, $B$ has a least element, say $\ell$. By the definition of $B$, for every $k\leq t<\ell, S(t)$ is true. The premise of the inductive hypothesis is true, and so $S(\ell)$ is true, contradicting that $\ell\in B$. Hence $B=\varnothing$. $\blacksquare$ Proof of weak induction from strong: Assume that strong induction holds (in particular, for $k=1$). That is, assume that if $S(1)$ is true and for every $m\geq 1, [S(1)\land S(2)\land\cdots\land S(m)]\to S(m+1)$, then for every $n\geq 1, S(n)$ is true. Observe (by truth tables, if desired), that for $m+1$ statements $p_i$, $$ [p_1\to p_2]\land[p_2\to p_3]\land\cdots\land[p_m\to p_{m+1}]\Rightarrow[(p_1\land p_2\land\cdots\land p_m)\to p_{m+1}],\tag{$\dagger$} $$ itself a result provable by induction (see end of answer for such a proof). Assume that the hypotheses of weak induction are true, that is, that $S(1)$ is true, and that for arbitrary $t, S(t)\to S(t+1)$. By repeated application of these recent assumptions, $S(1)\to S(2), S(2)\to S(3),\ldots, S(m)\to S(m+1)$ each hold. By the above observation, then $$ [S(1)\land S(2)\land\cdots\land S(m)]\to S(m+1). $$ Thus the hypotheses of strong induction are complete, and so one concludes that for every $n\geq 1$, the statement $S(n)$ is true, the consequence desired to complete the proof of weak induction. $\blacksquare$ Proving any positive integer $n\geq 2$ is a product of primes using strong induction: Let $S(n)$ be the statement "$n$ is a product of primes." Base step ($n=2$): Since $n=2$ is trivially a product of primes (actually one prime, really), $S(2)$ is true. Inductive step: Fix some $m\geq 2$, and assume that for every $t$ satisfying $2\leq t\leq m$, the statement $S(t)$ is true. To be shown is that $$ S(m+1) : m+1 \text{ is a product of primes}, $$ is true. If $m+1$ is a prime, then $S(m+1)$ is true. If $m+1$ is not prime, then there exist $r$ and $s$ with $2\leq r\leq m$ and $2\leq s\leq m$ so that $m+1=rs$. Since $S(r)$ is assumed to be true, $r$ is a product of primes [note: This is where it is imperative that we use strong induction; using weak induction, we cannot assume $S(r)$ is true]; similarly, by $S(s), s$ is a product of primes. Hence $m+1=rs$ is a product of primes, and so $S(m+1)$ holds. Thus, in either case, $S(m+1)$ holds, completing the inductive step. By mathematical induction, for all $n\geq 2$, the statement $S(n)$ is true. $\blacksquare$ Proof of $1+2+3+\cdots+n = \frac{n(n+1)}{2}$ by strong induction: Using strong induction here is completely unnecessary, for you do not need it at all, and it is only likely to confuse people as to why you are using it. It will proceed just like a proof by weak induction, but the assumption at the outset will look different; nonetheless, just to show what I am talking about, I will prove it using strong induction. Let $S(n)$ denote the proposition $$ S(n) : 1+2+3+\cdots+n = \frac{n(n+1)}{2}. $$ Base step ($n=1$): $S(1)$ is true because $1=\frac{1(1+1)}{2}$. Inductive step: Fix some $k\geq 1$, and assume that for every $t$ satisfying $1\leq t\leq k$, the statement $S(t)$ is true. To be shown is that $$ S(k+1) : 1+2+3+\cdots+k+(k+1)=\frac{(k+1)(k+2)}{2} $$ follows. Beginning with the left-hand side of $S(k+1)$, \begin{align} \text{LHS} &= 1+2+3+\cdots+k+(k+1)\tag{by definition}\\[1em] &= (1+2+3+\cdots+k)+(k+1)\tag{group terms}\\[1em] &= \frac{k(k+1)}{2}+(k+1)\tag{by $S(k)$}\\[1em] &= (k+1)\left(\frac{k}{2}+1\right)\tag{factor out $k+1$}\\[1em] &= (k+1)\left(\frac{k+2}{2}\right)\tag{common denominator}\\[1em] &= \frac{(k+1)(k+2)}{2}\tag{desired expression}\\[1em] &= \text{RHS}, \end{align} we obtain the right-hand side of $S(k+1)$. By mathematical induction, for all $n\geq 1$, the statement $S(n)$ is true. $\blacksquare$ $\color{red}{\text{Comment:}}$ See how this was really no different than how a proof by weak induction would work? The only thing different is really an unnecessary assumption made at the beginning of the proof. However, in your prime number proof, strong induction is essential; otherwise, we cannot assume $S(r)$ or $S(s)$ to be true. Here, any assumption regarding $t$ where $1\leq t\leq k$ is really useless because we don't actually use it anywhere in the proof, whereas we did use the assumptions $S(r)$ and $S(s)$ in the prime number proof, where $1\leq t\leq m$, because $r,s < m$. Does it now make sense why it was necessary to use strong induction in the prime number proof? Proof of $(\dagger)$ by induction: For statements $p_1,\ldots,p_{m+1}$, we have that $$ [p_1\to p_2]\land[p_2\to p_3]\land\cdots\land[p_m\to p_{m+1}]\Rightarrow[(p_1\land p_2\land\cdots\land p_m)\to p_{m+1}]. $$ Proof. For each $m\in\mathbb{Z^+}$, let $S(m)$ be the statement that for $m+1$ statements $p_i$, $$ S(m) : [p_1\to p_2]\land[p_2\to p_3]\land\cdots\land[p_m\to p_{m+1}]\Rightarrow[(p_1\land p_2\land\cdots\land p_m)\to p_{m+1}]. $$ Base step: The statement $S(1)$ says $$ [p_1\to p_2]\Rightarrow [(p_1\land p_2)\to p_2], $$ which is true (since the right side is a tautology). Inductive step: Fix $k\geq 1$, and assume that for any statements $q_1,\ldots,q_{k+1}$, both $$ S(1) : [q_1\to q_2]\Rightarrow [(q_1\land q_2)\to q_2] $$ and $$ S(k) : [q_1\to q_2]\land[q_2\to q_3]\land\cdots\land[q_k\to q_{k+1}]\Rightarrow[(q_1\land q_2\land\cdots\land q_k)\to q_{k+1}]. $$ hold. It remains to show that for any statements $p_1,p_2,\ldots,p_k,p_{k+1},p_{k+2}$ that $$ S(k+1) : [p_1\to p_2]\land[p_2\to p_3]\land\cdots\land[p_{k+1}\to p_{k+2}]\Rightarrow[(p_1\land p_2\land\cdots\land p_{k+1})\to p_{k+2}] $$ follows. Beginning with the left-hand side of $S(k+1)$, \begin{align} \text{LHS} &\equiv [p_1\to p_2]\land\cdots\land[p_{k+1}\to p_{k+2}]\land[p_{k+1}\to p_{k+2}]\\[0.5em] &\Downarrow\qquad \text{(definition of conjunction)}\\[0.5em] &[[p_1\to p_2]\land[p_2\to p_3]\land\cdots\land[p_{k+1}\to p_{k+2}]]\land[p_{k+1}\to p_{k+2}]\\[0.5em] &\Downarrow\qquad \text{(by $S(k)$ with each $q_i = p_i$)}\\[0.5em] &[(p_1\land p_2\land\cdots\land p_k)\to p_{k+1}]\land[p_{k+1}\to p_{k+2}]\\[0.5em] &\Downarrow\qquad \text{(by $S(1)$ with $q_1=p_1\land\cdots\land p_k)$ and $q_2=p_{k+1}$)}\\[0.5em] &[[(p_1\land p_2\land\cdots\land p_k)\land p_{k+1}]\to p_{k+1}]\land [p_{k+1}\to p_{k+2}]\\[0.5em] &\Downarrow\qquad \text{(by definition of conjunction)}\\[0.5em] &[(p_1\land p_2\land\cdots\land p_k\land p_{k+1}]\to p_{k+1}]\land [p_{k+1}\to p_{k+2}]\\[0.5em] &\Downarrow\qquad \text{(since $a\land b\to b$ with $b=[p_{k+1}\to p_{k+2}]$)}\\[0.5em] &[(p_1\land p_2\land\cdots\land p_k\land p_{k+1})\to p_{k+2}]\land[p_{k+1}\to p_{k+2}]\\[0.5em] &\Downarrow\qquad \text{(since $a\land b\to a$)}\\[0.5em] &(p_1\land p_2\land\cdots\land p_k\land p_{k+1})\to p_{k+2}\\[0.5em] &\equiv \text{RHS}, \end{align} we obtain the right-hand side of $S(k+1)$, which completes the inductive step. By mathematical induction, for each $n\geq 1, S(n)$ holds. $\blacksquare$

\newpage

\section{Where is the flaw in this \textbackslash{}&quot;proof\textbackslash{}&quot; of the Collatz Conjecture?}
\textit{Source: Stackexchange}

\subsection*{Question}
Edit I've highlighted the area in the proof where the mistake was made, for the benefit of anyone stumbling upon this in the future. It's the same mistake, made in two places: This has proven the Collatz Conjecture for all even numbers The Collatz Conjecture was shown to hold for $N+1$ when $N+1$ is even -- it was never shown to hold for all even numbers -- just that one, lone even number. [The Collatz Conjecture holds for] all odd numbers for which $N-1$ is a multiple of $4$ The same as above: it was shown that the Collatz Conjecture holds for $N+1$ if $N+1$ is of the form $4k+1$. It was never shown to hold for all numbers of this form -- just that one, lone number. In order for my proof to be valid, I would need to prove that the Collatz Conjecture holds for $N+1+4j = 4k+1$ (every fourth number after $N+1$) for, at a minimum, $N+1+4j  3$ By the induction hypothesis, the Collatz Conjecture holds for $N+1$ when $N+1 = 2k$ Now the last obvious bit: If $N$ is even, $N+1$ is odd If $N+1$ is odd, the next number in the series is 3(N+1)+1 Since $(N+1)$ is odd, $3(N+1)+1$ is even The next next number in the series is $(3(N+1)+1)/2$ This simplifies to: $(3N + 4)/2 = 1.5N + 2$ Now the first tricky bit: If $N$ is a multiple of $4$: $1.5N$ is a multiple of $6$, and therefore even. $1.5N + 2$ is therefore even The next next next number in the series is therefore $(1.5N+2)/2$ This simplifies to $0.75N + 1$ This is less than $N$ for $N > 4$ By the induction hypothesis, the Collatz Conjecture holds for $N+1$ when $N+1 = 4k + 1$ This has proven the Collatz Conjecture for all even numbers and all odd numbers for which $N-1$ is a multiple of $4$... Now to blow your minds: Breaking out of formal equations into patterns and such since I didn't know how to formalize this bit with math symbols: We now know that a number $N+1$ can ONLY violate the Collatz Conjecture if $N$ is even and not a multiple of $4$. In other words, the only way a number could potentially violate the Collatz Conjecture is if it's of the form $N+1 = 4k - 1$ This limits our numbers to test to 2+1, 6+1, 10+1, 14+1, 18+1, 22+1, etc. (note that I wrote these numbers in "$N+1$" format so it'd be simpler to apply the $1.5N+2$ shortcut) We'll apply our $1.5N + 2$ shortcut to a handful of these numbers: $2 -> 3+2 = 5 (4 +1) -- 4 is a multiple of 4 (duh) 6 -> 9+2 = 11 (10+1) 10 -> 15+2 = 17 (16+1) -- 16 is a multiple of 4 14 -> 21+2 = 23 (22+1) 18 -> 27+2 = 29 (28+1) -- 28 is a multiple of 4 22 -> 33+2 = 35 (34+1) 26 -> 39+2 = 41 (40+1) -- 40 is a multiple of 4 30 -> 45+2 = 47 (46+1) 34 -> 51+2 = 53 (52+1) -- 52 is a multiple of 4 38 -> 57+2 = 59 (58+1) 42 -> 63+2 = 65 (64+1) -- 64 is a multiple of 4 46 -> 69+2 = 71 (70+1) $ Every other line we automatically know the Collatz Conjecture will hold, because we've hit a number that can be expressed as $4k+1$ Looking at the "kept" rows, we can see that all we need to test now are numbers of the form: $N+1 = 8k - 1$ (in other words, the rows where $N = 8k - 2$ -- 6, 14, 22, etc.) And finally, recurse on this solution by drawing a new table and instead of computing the "next next" value, compute the "next next next next" value: "Next next next" value = 3(1.5N + 2) + 1 = 4.5N + 7 "next^4" value is half of this -- 2.25N + 3.5 $6 -> 27 +7 = 34 -> 17 (16 +1) -- 16 is a multiple of 4 14 -> 63 +7 = 70 -> 35 (34 +1) 22 -> 99 +7 = 106 -> 53 (52 +1) -- 52 is a multiple of 4 30 -> 135+7 = 142 -> 71 (70 +1) 38 -> 171+7 = 178 -> 89 (88 +1) -- 88 is a multiple of 4 46 -> 207+7 = 214 -> 107 (106+1) 54 -> 243+7 = 250 -> 125 (124+1) -- 124 is a multiple of 4 62 -> 279+7 = 286 -> 143 (142+1) $ Every other line we automatically know the Collatz Conjecture will hold, because we've hit a number that can be expressed as 4k+1 We now know a number can only violate the Collatz Conjecture if it's of the form: $N+1 = 16k - 1$... Recurse again: "next^5" value is 3(2.25N + 3.5) + 1 = 6.75N + 11.5 "next^6" value is (6.75N + 11.5)/2 = 3.375N + 5.75 $14 -> 53 = 52 + 1 -- 52 is a multiple of 4 30 -> 107 = 106 + 1 46 -> 161 = 160 + 1 -- 160 is a multiple of 4 62 -> 215 = 214 + 1 78 -> 269 = 268 + 1 -- 268 is a multiple of 4 94 -> 323 = 322 + 1 110 -> 377 = 376 + 1 -- 376 is a multiple of 4 126 -> 431 = 430 + 1 $ We now know a number can only violate the Collatz Conjecture if it's of the form $N+1 = 32k - 1$ At this point, a pattern is quickly emerging: First, a number could only violate the Collatz Conjecture if it was of the form $N+1 = 4k - 1$ Next, a number was shown that it could only violate the Collatz Conjecture if it was of the form $N+1 = 8k - 1$ Next, a number was shown that it could only violate the Collatz Conjecture if it is of the form $N+1 = 16k - 1$ Now, a number has been shown that it can only violate the Collatz Conjecture if it is of the form $N+1 = 32k - 1$ I've continued this process (recursively building this table and removing rows that I know cannot violate the Collatz Conjecture since they can be expressed as $4k+1$) all the way up until $512k - 1$ by hand. I do not know how to formalize this final process in mathematical notation, but I believe it demonstrates at least a viable method for proving the Collatz Conjecture. For every two steps we take into the Collatz series, we increase the power on our definition of "only numbers that could possibly violate the conjecture". Therefore for an arbitrarily large power we know that the conjecture will still hold. For Fun To help me in building these tables, I crafted the following Python script: $# Increment this variable to recurse one level deeper test = 1 ### No need to edit below here, but feel free to read it ### depth = 2 * test step = 2 ** (test + 1) start = step - 1 for x in range(0, 20): num = start + x * step _num = num _depth = depth while _depth > 0: if _num % 2 == 0: _num = _num / 2 else: _num = 3 * _num + 1 _depth -= 1 text = "" if (_num - 1) % 4 == 0: text = "-- multiple of 4" print "%s: %s = %s + 1 %s" % (num - 1, _num, _num - 1, text) $

\subsection*{Answer}
There is a subtle issue with your induction argument: you are assuming that the Collatz conjecture holds for all integers $\leq n$, and then want to prove it holds for $n+1$ (strong induction). So far, so good. You then prove that for some cases ($n+1$ even, or of the form $4k+1$) that the Collatz conjecture holds by the inductive hypothesis. Fine. You then try to argue that for some numbers of the form $4k+3$, you eventually hit a number of the form $4k+1$, so that the Collatz conjecture holds... not so fast. You haven't proven that the Collatz conjecture holds for all integers of the form $4k+1$. You've proven it's true for $n+1$, if $n+1$ happens to be of that form, and you've assumed it's true for all numbers of that form $\leq n$ (by the inductive hypothesis) but you haven't shown that Collatz holds for numbers of the form $4k+1$ that are larger than $n+1$.

\newpage

\section{Limit of $(1+ x/n)^n$ when $n$ tends to infinity}
\textit{Source: Stackexchange}

\subsection*{Question}
Does anyone know the exact proof of this limit result? $$\lim_{n\to\infty} \left(1+\frac{x}{n}\right)^n = e^x$$

\subsection*{Answer}
A short proof: $\left(1+\frac{x}{n}\right)^n = e^{n\log\left(1+\dfrac{x}{n}\right)}$ Since $\log(1+x) = x + O(x^2)$ when $x \to 0$, we have $n\log(1 + \frac{x}{n}) = x + O(\frac{x^2}{n})$ when $n\to +\infty$

\newpage

\section{Systems of linear equations: Why does no one plug back in?}
\textit{Source: Stackexchange}

\subsection*{Question}
When someone wants to solve a system of linear equations like $$\begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}\,,$$ they might use this logic: $$\begin{align} \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases} \iff &\begin{cases} -2x-y=0 \\ 3x+y=4 \end{cases} \\ \color{maroon}{\implies} &\begin{cases} -2x-y=0\\ x=4 \end{cases} \iff \begin{cases} -2(4)-y=0\\ x=4 \end{cases} \iff \begin{cases} y=-8\\ x=4 \end{cases} \,.\end{align}$$ Then they conclude that $(x, y) = (4, -8)$ is a solution to the system. This turns out to be correct, but the logic seems flawed to me. As I see it, all this proves is that $$ \forall{x,y\in\mathbb{R}}\quad \bigg( \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases} \color{maroon}{\implies} \begin{cases} y=-8\\ x=4 \end{cases} \bigg)\,. $$ But this statement leaves the possibility open that there is no pair $(x, y)$ in $\mathbb{R}^2$ that satisfies the system of equations. $$ \text{What if}\; \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases} \;\text{has no solution?} $$ It seems to me that to really be sure we've solved the equation, we have to plug back in for $x$ and $y$. I'm not talking about checking our work for simple mistakes. This seems like a matter of logical necessity. But of course, most people don't bother to plug back in, and it never seems to backfire on them. So why does no one plug back in? P.S. It would be great if I could understand this for systems of two variables, but I would be deeply thrilled to understand it for systems of $n$ variables. I'm starting to use Gaussian elimination on big systems in my linear algebra class, where intuition is weaker and calculations are more complex, and still no one feels the need to plug back in.

\subsection*{Answer}
You wrote this step as an implication: $$\begin{cases} -2x-y=0 \\ 3x+y=4 \end{cases} \implies \begin{cases} -2x-y=0\\ x=4 \end{cases}$$ But it is in fact an equivalence: $$\begin{cases} -2x-y=0 \\ 3x+y=4 \end{cases} \iff \begin{cases} -2x-y=0\\ x=4 \end{cases}$$ Then you have equivalences end-to-end and, as long as all steps are equivalences, you proved that the initial equations are equivalent to the end solutions, so you don't need to "plug back" and verify. Of course, carefulness is required to ensure that every step is in fact reversible.

\newpage

\section{The \textbackslash{}&\textbackslash{}#39;Factorialth Root\textbackslash{}&\textbackslash{}#39;}
\textit{Source: Stackexchange}

\subsection*{Question}
I was dealing with the following question, given by my friend: Let $\xi(x)=\sqrt{x+\sqrt{x+\sqrt{x+\sqrt{\cdots}}}}$ Define the series $X$ as $\xi(1),\xi(2),\xi(3),\dots$ Find $n$ for which $\xi(n)$ is the 51st Whole Number in the series. I solved it, of course, [and interestingly $\xi(1)={{1+\sqrt5}\over2}$, the Golden Ratio] but that led us on a competition in which we would try to find out the value of increasingly convoluted expressions. Some time later, I made an expression, which I called 'The Factorialth Root', written as $\sqrt[!]{x}$. For some $x$, $\sqrt[!]{x}=\sqrt{x\sqrt{(x-1)\sqrt{(x-2)\sqrt{\ddots\sqrt{2\sqrt1}}}}}$ My friend thought that $(x>y)\to(\sqrt[!]{x}y)\to(\sqrt[!]{x}>\sqrt[!]{y})$. I showed by example that mine was correct, but couldn't prove it. My attempt: If $[(x>y)\to(\sqrt[!]{x}>\sqrt[!]{y})]$ is true, then $\sqrt[!]{x}>\sqrt[!]{x-1}$. This is possible only when $x>\sqrt[!]{x-1}$. It follows that $\sqrt[!]{2}>\sqrt[!]{1},\sqrt[!]{3}>\sqrt[!]{2}$, and so on. So, I thought I could prove it by induction, but can't seem to find any way to apply it. Can anyone help?

\subsection*{Answer}
For $n > 1$, $$\begin{align} \frac{\sqrt[!]{n}}{\sqrt[!]{n-1}} &= \frac{n^{1/2} \cdot (n-1)^{1/4} \cdot (n-2)^{1/8} \cdot\,\cdots\,\cdot 1^{1/2^{n\phantom{-1}}}}{\phantom{n^{1/2}\cdot}(n-1)^{1/2}\cdot(n-2)^{1/4}\cdot\,\cdots\,\cdot 1^{1/2^{n-1}}} \\[4pt] &= \frac{n^{1/2}}{(n-1)^{1/4}\cdot(n-2)^{1/8}\cdot\,\cdots\,\cdot 1^{1/2^n}} \\[4pt] &= \frac{n^{1/4+1/8+1/16+\cdots+1/2^{n}+1/2^{n}}}{(n-1)^{1/4}\cdot(n-2)^{1/8}\cdot\,\cdots\,\cdot 1^{1/2^n}} \\[4pt] &= \left(\frac{n}{n-1}\right)^{1/4}\left(\frac{n}{n-2}\right)^{1/8}\left(\frac{n}{n-3}\right)^{1/16}\cdot\,\cdots\,\cdot \left(\frac{n}{1}\right)^{1/2^{n}}\cdot n^{1/2^n} \\[4pt] &> 1 \cdot 1 \cdot 1 \cdot\,\cdots\,\cdot1 \cdot 1 \\[4pt] &= 1 \end{align}$$

\newpage

\section{What really is mathematical rigor? How can I be more rigorous?}
\textit{Source: Stackexchange}

\subsection*{Question}
I'm an undergraduate mathematics student who has received some constructive feedback from two instructors at the end of my exams. Namely, that I am a bit hand-wavey and not always very rigorous. While I greatly appreciate this feedback since I intend to apply to graduate school, I worry because when I hand in assignment, I generally feel the proofs are rigorous already, and I'm not sure what I'm missing. Obviously I will be engaging in a dialogue with my professors about what I can do to be more rigorous, but I'm hoping the experts on this forum can provide some examples or distinctions between rigorous arguments and arguments that are close, but maybe gloss over important details so I can more clearly see the difference. It is worth noting that I have taken a course in mathematical reasoning and logic and did very well in it. Something has happened in the last year or so to reduce the quality of my arguments, or rather, the expectation has gone up and my level of rigor has not matched it. I think another aspect of what makes this a troublesome problem for me is that i typically do really well on homework problem sets. 90% consistently, sometimes with little mistakes. I feel like I am getting mixed signals from some of my classes when I am consistently told I am doing well, but am given this advice. All of this is said without spite or malice, I just sometimes feel confused about my own level of understanding. So if anyone has any general advice, or useful examples of arguments that look rigorous but need to be patched up, that would be greatly appreciated. I want to get a sense for what a really solid proof looks like compared to a less polished argument that looks passable to someone still with naivete in them. Edit: Here is an example of one such question in which I struggled for a long while to produce the proof posted, and even then I was missing something to be fully rigorous. Homology groups of orientable surfaces.

\subsection*{Answer}
You should be able to delineate the precise mathematical theorems that allow you to make each step in a proof. For example, if you have $(x,y) \in \mathbb{R}^2$ and you write: let $r,\theta$ satisfy $x = r\cos \theta,y=r\sin \theta$ with $r\geq 0$ and $2\pi > \theta \geq 0$, you are using a theorem that says that: Proposition. For all $x,y \in \mathbb{R}$, there exists $r \in \mathbb{R}_{\geq 0}$ and $\theta \in \mathbb{R}$ such that $x=r\cos \theta$ and $y = r \sin \theta$ and $2\pi > \theta \geq 0$. Make sure you can explicitly write out the theorems that allow you to make the steps you are making. The other thing is that you need to develop an intuition for what the instructor (reader, etc.) expects you to take for granted. You may need to write some follow-up lemmas if there are steps in your proof which invoke theorems that you really shouldn't be taking for granted.

\newpage

\section{When working proof exercises from a textbook with no solutions manual, how do you know when your proof is sound/acceptable?}
\textit{Source: Stackexchange}

\subsection*{Question}
When working proof exercises from a textbook with no solutions manual, how do you know when your proof is sound/acceptable? Often times I "feel" as if I can write a proof to an exercise but most of those times I do not feel confident that the proof that I am thinking of is good enough or even correct at all. I can sort of think a proof in my head, but am not confident this is a correct proof. Any input would be appreciated. Thanks.

\subsection*{Answer}
Ask a more experienced person. IMHO that's really the only option, and one of the reasons for this is that it is very important for a proof to communicate a result and its justification to another person. If the proof is good enough to convince yourself, that's a start, but the real test is whether you can express it in such a way as to convince someone else. And BTW... the same applies if the textbook does have a solutions manual. Your proof is inevitably going to be different from the one in the book, and it takes a lot of experience and mathematical understanding to decide whether the differences are important or not.

\newpage

\section{Why do we write proofs \textbackslash{}&quot;forward?\textbackslash{}&quot;}
\textit{Source: Stackexchange}

\subsection*{Question}
I am aware that this might turn into a discussion, but I have a feeling this might have an answer (maybe something historical?) instead. I'm hoping that those with speculations keep it in the comments. I have started to work on formal proof writing this quarter, and I discovered that the key to getting to some of them is to think of the problem "backwards." But, alas, when I wrote my proof starting with this, my professor said I shouldn't do it. But why not? It gives the reader a sense of what motivated this type of proof and allows for more understanding, doesn't it? Mods: Feel free to close, if this turns out to be too much of a discussion. I will be in chat for those willing to discuss this.

\subsection*{Answer}
One main problem with writing an argument backwards, especially for a student beginning to learn about proofs, is that it would be much more difficult to keep track of what is an assumption and what is a goal. In a proof that $A\implies B$, we should never along the way assume that $B$ is true, otherwise we are being circular; but if the statement of $B$ is written down on your paper already, you might get confused and think you'd already demonstrated it to be true. I'm not saying this will always happen, just that it is a greater risk. While it's true that "thinking backwards" can sometimes be a useful strategy for attacking a problem, and explaining your strategy to the reader can be a good addition to a formal proof, it is not a substitute; one should always be able to explain the argument starting from your given information and axioms, and proceeding to the desired statement completely "forwards". It is essential to get sufficient practice with phrasing your argument this way.

\newpage

\section{What is the proof that covariance matrices are always semi-definite?}
\textit{Source: Stackexchange}

\subsection*{Question}
Suppose that we have two different discreet signal vectors of $N^\text{th}$ dimension, namely $\mathbf{x}[i]$ and $\mathbf{y}[i]$, each one having a total of $M$ set of samples/vectors. $\mathbf{x}[m] = [x_{m,1} \,\,\,\,\, x_{m,2} \,\,\,\,\, x_{m,3} \,\,\,\,\, ... \,\,\,\,\, x_{m,N}]^\text{T}; \,\,\,\,\,\,\, 1 \leq m \leq M$ $\mathbf{y}[m] = [y_{m,1} \,\,\,\,\, y_{m,2} \,\,\,\,\, y_{m,3} \,\,\,\,\, ... \,\,\,\,\, y_{m,N}]^\text{T}; \,\,\,\,\,\,\,\,\, 1 \leq m \leq M$ And, I build up a covariance matrix in-between these signals. $\{C\}_{ij} = E\left\{(\mathbf{x}[i] - \bar{\mathbf{x}}[i])^\text{T}(\mathbf{y}[j] - \bar{\mathbf{y}}[j])\right\}; \,\,\,\,\,\,\,\,\,\,\,\, 1 \leq i,j \leq M $ Where, $E\{\}$ is the "expected value" operator. What is the proof that, for all arbitrary values of $\mathbf{x}$ and $\mathbf{y}$ vector sets, the covariance matrix $C$ is always semi-definite ($C \succeq0$) (i.e.; not negative definte; all of its eigenvalues are non-negative)?

\subsection*{Answer}
A symmetric matrix $C$ of size $n\times n$ is semi-definite if and only if $u^tCu\geqslant0$ for every $n\times1$ (column) vector $u$, where $u^t$ is the $1\times n$ transposed (line) vector. If $C$ is a covariance matrix in the sense that $C=\mathrm E(XX^t)$ for some $n\times 1$ random vector $X$, then the linearity of the expectation yields that $u^tCu=\mathrm E(Z_u^2)$, where $Z_u=u^tX$ is a real valued random variable, in particular $u^tCu\geqslant0$ for every $u$. If $C=\mathrm E(XY^t)$ for two centered random vectors $X$ and $Y$, then $u^tCu=\mathrm E(Z_uT_u)$ where $Z_u=u^tX$ and $T_u=u^tY$ are two real valued centered random variables. Thus, there is no reason to expect that $u^tCu\geqslant0$ for every $u$ (and, indeed, $Y=-X$ provides a counterexample).

\newpage

\section{Proof for triangle inequality for vectors}
\textit{Source: Stackexchange}

\subsection*{Question}
Generally,the length of the sum of two vectors is not equal to the sum of their lengths. To see this consider the vectors $u$ and $v$ as shown below. By considering $u$ and $v$ as two sides of a triangle, we can see that the lengths of the third side is $\| u + v \|$ and we have $\| u + v \| \leq \|u\| + \|v\|$. Under what circumstance equality occurs and how can one prove that?

\subsection*{Answer}
I have noticed that the answer has been written down in the comments. Just to have an answer I am writing this one down. Consider $\|u+v\|^2=(u+v) \cdot (u+v)$ where $u \cdot v$ represents the standard inner product/scalar product.Therefore $$\|u+v\|^2=\|u\|^2+2 (u \cdot v) + \|v\|^2 .$$ By the Cauchy-Schwarz Inequality we have $$u \cdot v \leq \|u\| \cdot \|v\|.$$ So, $$\|u+v \|^2= \|u\|^2+2(u \cdot v)+ \|v \|^2 \leq \|u\|^2+ 2 \|u\| \cdot \|v\| + \|v\|^2=(\|u\|+ \|v\|)^2 ,$$ i.e., $$\|u+v\|^2 \leq (\|u\|+ \|v\|)^2 \implies \|u+v\| \leq \|u \|+ \|v\| .$$ The Cauchy-Schwarz Inequality holds for any inner Product, so the triangle inequality holds irrespective of how you define the norm of the vector to be, i.e., the way you define scalar product in that vector space. In this case, the equality holds when vectors are parallel i.e, $u=kv$, $k \in \mathbb{R}^+$ because $u \cdot v= \|u \| \cdot \|v\| \cos \theta$ when $\cos \theta=1$, the equality of the Cauchy-Schwarz inequality holds.

\newpage

\end{document}