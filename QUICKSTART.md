# Guide de D√©marrage Rapide

## ‚ö° Quick Start (5 minutes)

### 1. Installation

```bash
# Cloner le projet
cd math_scraper

# Cr√©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# OU: venv\Scripts\activate  # Windows

# Installer d√©pendances
pip install -r requirements.txt
```

### 2. Test rapide

```bash
# Tester que tout fonctionne
python test.py
```

Cela va:
- Tester chaque scraper individuellement
- V√©rifier le nettoyage des donn√©es
- Tester le syst√®me de stockage
- Devrait prendre ~2-3 minutes

### 3. Premier scraping (√©chantillon)

```bash
# Collecter ~1000 items (2-3 minutes)
python main.py
```

Cela va cr√©er:
```
math_dataset/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ stackexchange/batch_*.json
‚îÇ   ‚îî‚îÄ‚îÄ proofwiki/batch_*.json
‚îî‚îÄ‚îÄ index.json
```

### 4. Analyser les donn√©es

```bash
python analyze.py
```

Affiche:
- Nombre total d'items
- Distribution par source
- Tags populaires
- Structures de preuve
- Exemples

## üéØ Workflow complet

### Phase 1: Collecte (√©chantillon test)

```bash
python main.py
```

**R√©sultat attendu**: ~2000 items en 5 minutes

### Phase 2: Analyse

```bash
python analyze.py
```

**V√©rifier**:
- ‚úÖ Items avec formules math√©matiques > 80%
- ‚úÖ Items complets (question+r√©ponse) > 90%
- ‚úÖ Diversit√© des tags
- ‚úÖ Pas d'erreurs dans les logs

### Phase 3: Production (collecte compl√®te)

```bash
# √âchantillon rapide (5k items, ~10 min)
python production_scraping.py sample

# OU production compl√®te (700k items, plusieurs heures)
python production_scraping.py production
```

**Important**: Le scraping production peut √™tre interrompu avec Ctrl+C. Les donn√©es d√©j√† collect√©es seront conserv√©es.

### Phase 4: V√©rification finale

```bash
# Analyser dataset complet
python analyze.py

# V√©rifier les fichiers
ls -lh math_dataset/processed/
# Devrait afficher:
# - train.jsonl
# - validation.jsonl
# - test.jsonl
```

## üìä R√©sultats attendus

### √âchantillon (main.py par d√©faut)

```
Source              Items
----------------------------------
Stack Exchange      1,000
ProofWiki           1,000
----------------------------------
TOTAL               2,000
```

**Temps**: 5 minutes  
**Taille**: ~10 MB

### Production sample

```
Source              Items
----------------------------------
Stack Exchange      2,500
ProofWiki           2,500
----------------------------------
TOTAL               5,000
```

**Temps**: 10 minutes  
**Taille**: ~25 MB

### Production compl√®te

```
Source              Items
----------------------------------
Stack Exchange      500,000
ProofWiki           20,000
arXiv               50,000
Cours fran√ßais      30,000
----------------------------------
TOTAL               600,000+
```

**Temps**: 5-20 heures (selon connexion)  
**Taille**: ~2-3 GB

## üîß Personnalisation rapide

### Modifier les sources

Dans `main.py`:

```python
# Ne scraper que Stack Exchange
sources_to_scrape = ['stackexchange']

# Ou seulement sources fran√ßaises
sources_to_scrape = ['proofwiki', 'french_courses']
```

### Ajuster la quantit√©

```python
# Plus de donn√©es
await scraper.scrape_all(
    sources=sources_to_scrape,
    max_per_source=10000
)

# Moins de donn√©es (test rapide)
await scraper.scrape_all(
    sources=sources_to_scrape,
    max_per_source=100
)
```

### Filtrer par qualit√©

Dans `utils/cleaner.py`:

```python
# Plus strict (seulement contenu de tr√®s haute qualit√©)
cleaner = DataCleaner(
    min_length=200,  # Au lieu de 50
    max_length=3000  # Au lieu de 5000
)

# Moins strict (garder plus de contenu)
cleaner = DataCleaner(
    min_length=30,
    max_length=10000
)
```

## üêõ R√©solution de probl√®mes

### Erreur "No module named 'aiohttp'"

```bash
pip install -r requirements.txt
```

### Erreur "Rate limit exceeded" (Stack Exchange)

**Solution 1**: Attendre (quota se recharge)

**Solution 2**: Utiliser une cl√© API

```python
# Dans scrapers/stackexchange_scraper.py
scraper = StackExchangeScraper(api_key="VOTRE_CLE")
```

Obtenir cl√©: https://stackapps.com/apps/oauth/register

### Scraping tr√®s lent

**Causes possibles**:
- Connexion internet lente
- Rate limiting
- Serveur source surcharg√©

**Solutions**:
- Utiliser mode √©chantillon d'abord
- Lancer pendant la nuit
- R√©duire `max_per_source`

### Espace disque insuffisant

Production compl√®te n√©cessite ~3 GB.

**Solution**: Lib√©rer espace ou r√©duire quantit√©:

```python
max_per_source=50000  # Au lieu de None
```

## üìö Exemples de commandes

### Scraping progressif

```bash
# Jour 1: Stack Exchange
python -c "
import asyncio
from main import MathDataScraper

async def run():
    s = MathDataScraper('./dataset')
    await s.scrape_source('stackexchange', max_items=100000)

asyncio.run(run())
"

# Jour 2: ProofWiki
python -c "
import asyncio
from main import MathDataScraper

async def run():
    s = MathDataScraper('./dataset')  # M√™me dossier!
    await s.scrape_source('proofwiki', max_items=20000)

asyncio.run(run())
"
```

### Exporter donn√©es pour analyse

```bash
# Format CSV (pour Excel/Pandas)
python -c "
import json
import csv
from pathlib import Path

# Charger donn√©es
items = []
for f in Path('./math_dataset/raw').rglob('*.json'):
    items.extend(json.load(open(f)))

# Exporter CSV
with open('dataset.csv', 'w', encoding='utf-8') as f:
    if items:
        writer = csv.DictWriter(f, fieldnames=items[0].keys())
        writer.writeheader()
        writer.writerows(items)

print(f'Export√© {len(items)} items vers dataset.csv')
"
```

### Fusionner avec dataset existant

```python
from utils.storage import DataStorage

# Charger ancien dataset
old_storage = DataStorage('./old_dataset')

# Charger nouveau
new_storage = DataStorage('./new_dataset')

# Fusionner (anti-doublons automatique)
# Les IDs sont uniques, donc pas de doublons
```

## üéì Prochaines √©tapes

Apr√®s avoir collect√© les donn√©es:

1. **Exploration**: 
   ```bash
   python analyze.py
   jupyter notebook  # Si Jupyter install√©
   ```

2. **Pr√©paration pour Lean**:
   - Parser structure math√©matique
   - Cr√©er templates de conversion
   - Identifier patterns communs

3. **Fine-tuning**:
   - Charger avec Hugging Face `datasets`
   - Fine-tune CodeLlama/DeepSeek
   - Valider avec Lean compiler

4. **It√©ration**:
   - Analyser erreurs du mod√®le
   - Collecter plus de donn√©es cibl√©es
   - Affiner filtres de qualit√©

## üí° Astuces

- **Logs d√©taill√©s**: V√©rifier `scraping.log` pour diagnostiquer probl√®mes
- **Interruption s√ªre**: Ctrl+C sauvegarde les donn√©es d√©j√† collect√©es
- **Incr√©mental**: Peut lancer plusieurs fois, les doublons sont filtr√©s
- **Parall√®le**: Les scrapers tournent en parall√®le (plus rapide)

## ‚úÖ Checklist de r√©ussite

Avant de passer au fine-tuning, v√©rifier:

- [ ] Au moins 10k items collect√©s
- [ ] Plus de 80% avec formules math√©matiques
- [ ] Au moins 3 sources diff√©rentes
- [ ] Fichiers train/val/test cr√©√©s
- [ ] Pas d'erreurs critiques dans logs
- [ ] `analyze.py` montre bonne distribution

Si tous les points sont coch√©s: **Pr√™t pour l'entra√Ænement!** üéâ
