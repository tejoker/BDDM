"""
Configuration pour scraping en production
Collecte maximale de donn√©es (~700k items)
"""

import asyncio
from main import MathDataScraper


async def production_scraping():
    """
    Scraping complet pour production
    Dur√©e estim√©e: 5-20 heures selon connexion
    """
    
    print("üöÄ D√âMARRAGE SCRAPING PRODUCTION")
    print("=" * 60)
    print("\nConfiguration:")
    print("  - Toutes les sources activ√©es")
    print("  - Pas de limite d'items")
    print("  - Sauvegarde incr√©mentale")
    print("\n‚ö† Ce processus peut prendre plusieurs heures")
    print("  Interrompre avec Ctrl+C (donn√©es d√©j√† collect√©es seront conserv√©es)")
    print("\n" + "=" * 60 + "\n")
    
    # Initialiser scraper
    scraper = MathDataScraper(output_dir="./math_dataset_production")
    
    # Phase 1: Sources rapides et riches
    print("\nüì¶ PHASE 1: Sources rapides")
    print("  - Stack Exchange")
    print("  - ProofWiki")
    
    await scraper.scrape_all(
        sources=['stackexchange', 'proofwiki'],
        max_per_source=None  # Pas de limite
    )
    
    print("\n‚úì Phase 1 termin√©e")
    print(f"  Items collect√©s: {scraper.stats['total_scraped']}")
    
    # Phase 2: arXiv (plus lent)
    print("\nüìö PHASE 2: arXiv (peut prendre plusieurs heures)")
    
    await scraper.scrape_source('arxiv', max_items=50000)  # Limiter pour √©viter timeout
    
    print("\n‚úì Phase 2 termin√©e")
    
    # Phase 3: Cours fran√ßais
    print("\nüá´üá∑ PHASE 3: Cours fran√ßais")
    
    await scraper.scrape_source('french_courses', max_items=None)
    
    print("\n‚úì Phase 3 termin√©e")
    
    # R√©sum√© final
    print("\n" + "=" * 60)
    print("SCRAPING PRODUCTION TERMIN√â")
    print("=" * 60)
    
    summary = scraper.get_summary()
    print(f"\nüìä Statistiques finales:")
    print(f"  Total items: {summary['total_items']}")
    print(f"\n  Par source:")
    for source, count in summary['by_source'].items():
        print(f"    - {source}: {count:,} items")
    
    print(f"\n  Donn√©es sauvegard√©es: {summary['output_directory']}")
    
    # Post-processing
    print("\n‚öôÔ∏è  Post-processing...")
    storage = scraper.storage
    
    print("  - Fusion des batches...")
    storage.merge_to_single_file("complete_dataset.json")
    
    print("  - Cr√©ation des splits train/val/test...")
    storage.export_by_format()
    
    print("\n‚úÖ TOUT EST PR√äT POUR L'ENTRA√éNEMENT!")
    print(f"  Dataset complet: {summary['output_directory']}/complete_dataset.json")
    print(f"  Splits: {summary['output_directory']}/processed/")


async def quick_sample_scraping():
    """
    Version rapide pour tests/d√©mo
    ~10 minutes, ~5k items
    """
    print("‚ö° SCRAPING RAPIDE (√âCHANTILLON)")
    
    scraper = MathDataScraper(output_dir="./math_dataset_sample")
    
    await scraper.scrape_all(
        sources=['stackexchange', 'proofwiki'],
        max_per_source=2500  # 5k total
    )
    
    summary = scraper.get_summary()
    print(f"\n‚úì √âchantillon collect√©: {summary['total_items']} items")


async def custom_scraping():
    """
    Configuration personnalis√©e
    Adapter selon besoins
    """
    scraper = MathDataScraper(output_dir="./math_dataset_custom")
    
    # Exemple: seulement Stack Exchange, filtr√© par tags sp√©cifiques
    # (n√©cessite modification du scraper pour accepter des param√®tres)
    
    await scraper.scrape_source('stackexchange', max_items=10000)
    
    summary = scraper.get_summary()
    print(f"\n‚úì Custom scraping: {summary['total_items']} items")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
    else:
        print("Usage:")
        print("  python production_scraping.py [production|sample|custom]")
        print("\nModes:")
        print("  production - Scraping complet (700k+ items, plusieurs heures)")
        print("  sample     - √âchantillon rapide (5k items, ~10 min)")
        print("  custom     - Configuration personnalis√©e")
        sys.exit(1)
    
    if mode == "production":
        asyncio.run(production_scraping())
    elif mode == "sample":
        asyncio.run(quick_sample_scraping())
    elif mode == "custom":
        asyncio.run(custom_scraping())
    else:
        print(f"Mode inconnu: {mode}")
        sys.exit(1)
