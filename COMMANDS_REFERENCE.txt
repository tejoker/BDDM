ðŸŽ¯ RÃ‰FÃ‰RENCE RAPIDE DES COMMANDES
==================================

INSTALLATION
------------
# Linux/Mac
./install.sh

# Windows
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt


ACTIVATION ENVIRONNEMENT
-------------------------
# Linux/Mac
source venv/bin/activate

# Windows
venv\Scripts\activate


TESTS & VÃ‰RIFICATION
--------------------
# Test complet de tous les composants
python test.py

# Test d'un scraper spÃ©cifique
python scrapers/stackexchange_scraper.py
python scrapers/proofwiki_scraper.py


SCRAPING
--------
# Test rapide (~2k items, 5 min)
python main.py

# Ã‰chantillon (~5k items, 10 min)
python production_scraping.py sample

# Production complÃ¨te (600k+ items, heures)
python production_scraping.py production

# PersonnalisÃ© (modifier dans le fichier)
python production_scraping.py custom


ANALYSE
-------
# Statistiques complÃ¨tes du dataset
python analyze.py

# Voir structure des fichiers
ls -lh math_dataset/raw/
ls -lh math_dataset/processed/


MANIPULATION PYTHON
-------------------
# Charger et explorer donnÃ©es
python3 << 'PYTHON'
import json

# Charger un batch
with open('math_dataset/raw/stackexchange/batch_*.json') as f:
    data = json.load(f)
    print(f"{len(data)} items")
    print(data[0])  # Premier item
PYTHON

# Compter items par source
python3 << 'PYTHON'
import json
from pathlib import Path
from collections import Counter

items = []
for f in Path('math_dataset/raw').rglob('*.json'):
    items.extend(json.load(open(f)))

sources = Counter(item['source'] for item in items)
print(f"Total: {len(items)}")
for source, count in sources.items():
    print(f"  {source}: {count}")
PYTHON


NETTOYAGE
---------
# Supprimer dataset pour recommencer
rm -rf math_dataset/

# Supprimer logs
rm scraping.log

# Supprimer cache Python
find . -type d -name __pycache__ -exec rm -rf {} +


EXPORT PERSONNALISÃ‰
-------------------
# Fusionner tous les batches en un fichier
python3 << 'PYTHON'
from utils.storage import DataStorage

storage = DataStorage('./math_dataset')
storage.merge_to_single_file('all_data.json')
storage.export_by_format()
PYTHON

# Filtrer par langue
python3 << 'PYTHON'
import json
from pathlib import Path

# Charger tout
items = []
for f in Path('math_dataset/raw').rglob('*.json'):
    items.extend(json.load(open(f)))

# Filtrer franÃ§ais uniquement
fr_items = [item for item in items if item.get('language') == 'fr']

# Sauvegarder
with open('dataset_fr_only.json', 'w', encoding='utf-8') as f:
    json.dump(fr_items, f, indent=2, ensure_ascii=False)

print(f"Items franÃ§ais: {len(fr_items)}/{len(items)}")
PYTHON

# Filtrer par tag
python3 << 'PYTHON'
import json
from pathlib import Path

items = []
for f in Path('math_dataset/raw').rglob('*.json'):
    items.extend(json.load(open(f)))

# Seulement items avec tag "induction"
induction_items = [
    item for item in items 
    if 'induction' in item.get('tags', [])
]

print(f"Items avec induction: {len(induction_items)}")
PYTHON


MONITORING SCRAPING
-------------------
# Suivre logs en temps rÃ©el
tail -f scraping.log

# Voir statistiques pendant scraping
watch -n 5 'python3 -c "
from utils.storage import DataStorage
s = DataStorage(\"./math_dataset\")
print(s.get_stats())
"'

# Compter items collectÃ©s
find math_dataset/raw -name "*.json" -exec cat {} \; | grep -o '"id"' | wc -l


DÃ‰PANNAGE
---------
# VÃ©rifier version Python
python3 --version

# VÃ©rifier packages installÃ©s
pip list | grep -E "aiohttp|beautifulsoup|lxml"

# Tester connexion Stack Exchange API
curl "https://api.stackexchange.com/2.3/questions?site=math.stackexchange.com&pagesize=1"

# Tester accÃ¨s ProofWiki
curl -I https://proofwiki.org


OPTIMISATION
------------
# Augmenter limite Stack Exchange avec clÃ© API
# Ã‰diter scrapers/stackexchange_scraper.py:
# scraper = StackExchangeScraper(api_key="VOTRE_CLE")

# Ajuster filtres qualitÃ©
# Ã‰diter utils/cleaner.py:
# cleaner = DataCleaner(min_length=100, max_length=3000)

# Scraper moins/plus par source
# Dans main.py ou production_scraping.py:
# max_per_source=10000  # ou None pour tout


PRÃ‰PARATION POUR LEAN
---------------------
# AprÃ¨s scraping, prÃ©parer donnÃ©es pour fine-tuning

# 1. Analyser structures de preuve
python3 << 'PYTHON'
import json
from pathlib import Path
from collections import Counter

items = []
for f in Path('math_dataset/raw').rglob('*.json'):
    items.extend(json.load(open(f)))

structures = Counter()
for item in items:
    struct = item.get('proof_structure', {})
    ptype = struct.get('type', 'unknown')
    structures[ptype] += 1

print("Structures de preuve:")
for ptype, count in structures.most_common():
    print(f"  {ptype}: {count}")
PYTHON

# 2. Extraire patterns pour templates Lean
# TODO: Parser formules mathÃ©matiques
# TODO: Identifier correspondances Lean
# TODO: CrÃ©er paires (texte, code Lean)


WORKFLOW COMPLET
----------------
# 1. Installation
./install.sh

# 2. Test
python test.py

# 3. Scraping test
python main.py

# 4. VÃ©rification
python analyze.py

# 5. Production
python production_scraping.py production

# 6. Export final
python3 -c "
from utils.storage import DataStorage
s = DataStorage('./math_dataset')
s.export_by_format()
"

# 7. PrÃªt pour fine-tuning !
# Fichiers: math_dataset/processed/{train,validation,test}.jsonl


COMMANDES GIT
-------------
# Initialiser repo
git init
git add .
git commit -m "Initial commit: Math scraper"

# Pousser sur GitHub
git remote add origin YOUR_REPO_URL
git push -u origin main

# .gitignore dÃ©jÃ  configurÃ© pour ignorer:
# - math_dataset/
# - venv/
# - *.log
# - __pycache__/


ASTUCES
-------
# Lancer en arriÃ¨re-plan (Linux/Mac)
nohup python production_scraping.py production > output.log 2>&1 &

# VÃ©rifier processus
ps aux | grep python

# ArrÃªter proprement
pkill -SIGINT python  # ou Ctrl+C dans terminal

# Reprendre aprÃ¨s interruption
python production_scraping.py production
# (les doublons sont automatiquement Ã©vitÃ©s)


MÃ‰TRIQUES QUALITÃ‰
-----------------
# VÃ©rifier pourcentage avec formules math
python3 << 'PYTHON'
import json
from pathlib import Path

items = []
for f in Path('math_dataset/raw').rglob('*.json'):
    items.extend(json.load(open(f)))

with_math = sum(1 for item in items if '$' in str(item))
print(f"Avec formules: {with_math}/{len(items)} ({100*with_math/len(items):.1f}%)")
PYTHON

# VÃ©rifier items complets
python3 << 'PYTHON'
import json
from pathlib import Path

items = []
for f in Path('math_dataset/raw').rglob('*.json'):
    items.extend(json.load(open(f)))

complete = sum(1 for item in items if 
    ('question' in item and 'answer' in item) or
    ('theorem' in item and 'proof' in item))
    
print(f"Items complets: {complete}/{len(items)} ({100*complete/len(items):.1f}%)")
PYTHON


QUESTIONS FRÃ‰QUENTES
--------------------
Q: Combien de temps pour scraper tout ?
A: 5-20 heures selon connexion. Mode Ã©chantillon = 10 min.

Q: Puis-je interrompre ?
A: Oui ! Ctrl+C sauvegarde ce qui est collectÃ©.

Q: Ã‡a prend combien d'espace ?
A: ~3 GB pour production complÃ¨te.

Q: Comment Ã©viter les doublons ?
A: Automatique via index.json.

Q: Puis-je ajouter d'autres sources ?
A: Oui ! Voir ARCHITECTURE.md section "ExtensibilitÃ©".

Q: Rate limit dÃ©passÃ© ?
A: Attendre ou obtenir clÃ© API Stack Exchange.


RESSOURCES
----------
Documentation:
  - START_HERE.md       : Guide dÃ©marrage
  - README.md           : Documentation complÃ¨te  
  - QUICKSTART.md       : 5 minutes chrono
  - ARCHITECTURE.md     : DÃ©tails techniques
  - PROJECT_STRUCTURE.txt : Organisation projet
  - COMMANDS_REFERENCE.txt : Ce fichier

Liens utiles:
  - Stack Exchange API: https://api.stackexchange.com/docs
  - Lean docs: https://leanprover.github.io
  - DeepSeek-Prover: https://github.com/deepseek-ai/DeepSeek-Prover
