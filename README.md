# Math Data Scraper

Syst√®me de scraping modulaire pour collecter des donn√©es math√©matiques (th√©or√®mes, preuves, exercices) depuis plusieurs sources.

## üéØ Sources de donn√©es

1. **Stack Exchange Mathematics** (~500k items potentiels)
   - Questions avec r√©ponses accept√©es
   - Filtrage par score et tags
   - API officielle avec rate limiting

2. **ProofWiki** (~20k th√©or√®mes)
   - Th√©or√®mes formels avec preuves d√©taill√©es
   - Structure bien d√©finie
   - Excellent pour donn√©es structur√©es

3. **Wikipedia** (Articles math√©matiques)
   - Encyclop√©die accessible via API officielle
   - Articles sur concepts, th√©or√®mes, domaines math√©matiques
   - Extraits d'introduction en texte plain

4. **nLab** (Cat√©gorie theory & higher math)
   - Wiki de math√©matiques avanc√©es
   - Th√©orie des cat√©gories, topologie alg√©brique
   - D√©finitions rigoureuses et formelles

5. **MathOverflow** (Recherche niveau)
   - Questions/r√©ponses de niveau recherche
   - Utilise l'API Stack Exchange
   - Haute qualit√©, experts du domaine

6. **arXiv** (Papiers de recherche - optionnel)
   - M√©tadonn√©es de papiers math√©matiques
   - Titres, abstracts, auteurs
   - Cat√©gories: alg√®bre, analyse, g√©om√©trie, etc.

**Note**: Sources comme MathWorld, Art of Problem Solving et MIT OCW sont impl√©ment√©es mais peuvent √™tre bloqu√©es par protection anti-scraping.

## üì¶ Installation

```bash
# Cloner/t√©l√©charger le projet
cd math_scraper

# Cr√©er environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Installer d√©pendances
pip install -r requirements.txt
```

## üöÄ Utilisation rapide

### Collecter des √©chantillons (recommand√©)

```bash
# Collect 10 items from each main source
./math/bin/python collect_samples.py

# Collect custom amounts: SE PW ArXiv Wiki nLab MathOverflow
./math/bin/python collect_samples.py 20 15 0 10 5 10

# Example: 50 SE + 30 PW + 20 Wiki + 10 MathOverflow
./math/bin/python collect_samples.py 50 30 0 20 0 10
```

Les donn√©es sont sauvegard√©es dans `samples_en/raw/<source>/`

### Scraper toutes les sources (ancien mode)

```python
python main.py
```

Par d√©faut, cela scrape Stack Exchange et ProofWiki avec 1000 items maximum par source.

### Configuration personnalis√©e

```python
import asyncio
from main import MathDataScraper

async def custom_scrape():
    scraper = MathDataScraper(output_dir="./mon_dataset")
    
    # Choisir sources
    sources = ['stackexchange', 'proofwiki']
    
    # Scraper sans limite (production)
    await scraper.scrape_all(
        sources=sources,
        max_per_source=None  # Pas de limite
    )
    
    # Voir statistiques
    print(scraper.get_summary())

asyncio.run(custom_scrape())
```

### Tester un scraper individuellement

```bash
# Test Stack Exchange
python scrapers/stackexchange_scraper.py

# Test ProofWiki
python scrapers/proofwiki_scraper.py

# Test arXiv (plus lent)
python scrapers/arxiv_scraper.py
```

## üìä Structure des donn√©es

Chaque item scrapp√© a la structure suivante:

```json
{
  "id": "unique_identifier",
  "source": "stackexchange|proofwiki|arxiv|...",
  "title": "Titre du probl√®me/th√©or√®me",
  "question": "√ânonc√© (pour exercices)",
  "answer": "Solution (pour exercices)",
  "theorem": "√ânonc√© du th√©or√®me (pour preuves formelles)",
  "proof": "D√©monstration",
  "tags": ["algebra", "induction", ...],
  "url": "URL source",
  "language": "en|fr",
  "proof_structure": {
    "type": "induction|contradiction|direct",
    "techniques": ["mathematical_induction", ...],
    "steps": ["base_case", "inductive_step"]
  },
  "metadata": {...}
}
```

## üìÅ Organisation des fichiers

```
math_dataset/
‚îú‚îÄ‚îÄ raw/                    # Donn√©es brutes par source
‚îÇ   ‚îú‚îÄ‚îÄ stackexchange/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ batch_20241024_120000.json
‚îÇ   ‚îú‚îÄ‚îÄ proofwiki/
‚îÇ   ‚îî‚îÄ‚îÄ arxiv/
‚îú‚îÄ‚îÄ processed/              # Donn√©es nettoy√©es
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ validation.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ test.jsonl
‚îú‚îÄ‚îÄ index.json             # Index anti-doublons
‚îî‚îÄ‚îÄ scraping_stats.json    # Statistiques
```

## üîß Post-processing

### Fusionner tous les batches

```python
from utils.storage import DataStorage

storage = DataStorage("./math_dataset")
storage.merge_to_single_file("complete_dataset.json")
```

### Cr√©er splits train/val/test

```python
storage.export_by_format()
# Cr√©e automatiquement train.jsonl, validation.jsonl, test.jsonl
```

## ‚öôÔ∏è Configuration avanc√©e

### Rate Limiting

Les scrapers respectent les limites:
- Stack Exchange: 10k requ√™tes/jour (sans cl√© API)
- ProofWiki: 0.5s entre requ√™tes
- arXiv: 3s entre requ√™tes

### Ajouter une cl√© API Stack Exchange

Pour augmenter les limites (30k requ√™tes/jour):

```python
from scrapers.stackexchange_scraper import StackExchangeScraper

scraper = StackExchangeScraper(api_key="VOTRE_CLE")
```

Obtenir une cl√©: https://stackapps.com/apps/oauth/register

### Filtres de qualit√©

Dans `utils/cleaner.py`, vous pouvez ajuster:

```python
cleaner = DataCleaner(
    min_length=50,    # Longueur minimale
    max_length=5000   # Longueur maximale
)
```

## üìà Estimation de volum√©trie

Scraping complet (sans limite):
- Stack Exchange: ~500k items (‚âà2-3 heures)
- ProofWiki: ~20k items (‚âà1 heure)
- arXiv: ~100k preuves (‚âà10-15 heures, n√©cessite beaucoup de bande passante)

**Total estim√©: 600k+ items math√©matiques**

## üêõ D√©pannage

### Erreur "Rate limit exceeded"
Attendre ou utiliser une cl√© API.

### Erreur r√©seau/timeout
Augmenter le timeout dans les scrapers:
```python
async with session.get(url, timeout=60):
    ...
```

### M√©moire insuffisante
R√©duire `max_per_source` ou traiter par batches:
```python
await scraper.scrape_all(max_per_source=10000)
```

## üîú Prochaines √©tapes

Apr√®s le scraping, voici les √©tapes suivantes pour ton projet:

1. **Nettoyage suppl√©mentaire**: affiner les filtres de qualit√©
2. **Extraction de structure**: identifier patterns de preuve
3. **Mod√®le de traduction**: fine-tune pour LaTeX ‚Üí Lean
4. **Validation**: utiliser Lean compiler pour v√©rifier outputs

## üìù Notes importantes

- Les donn√©es proviennent de sources publiques mais v√©rifier les licences
- Stack Exchange: contenu sous CC BY-SA
- arXiv: acc√®s libre mais respecter conditions d'utilisation
- ProofWiki: licence Creative Commons

## ü§ù Contribution

Pour ajouter une nouvelle source:

1. Cr√©er `scrapers/nouvelle_source_scraper.py`
2. Impl√©menter `async def scrape(self, max_items: int) -> List[Dict]`
3. Ajouter dans `main.py` et `scrapers/__init__.py`

## üìß Support

Pour questions ou bugs, cr√©er une issue GitHub ou contacter directement.
