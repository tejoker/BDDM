STRUCTURE DU PROJET MATH SCRAPER
=====================================

📁 math_scraper/
│
├── 🚀 START_HERE.md              ← COMMENCER ICI !
├── 📖 QUICKSTART.md              Guide rapide 5 minutes
├── 📚 README.md                  Documentation complète
├── 🏗️  ARCHITECTURE.md            Explication technique détaillée
├── 📋 PROJECT_STRUCTURE.txt      Ce fichier
│
├── ⚙️ SCRIPTS PRINCIPAUX
│   ├── install.sh               Installation automatique
│   ├── main.py                  Orchestrateur principal (démarrer ici)
│   ├── test.py                  Tests unitaires
│   ├── analyze.py               Analyse du dataset collecté
│   └── production_scraping.py   Config pour scraping production
│
├── 🤖 SCRAPERS (scrapers/)
│   ├── __init__.py
│   ├── stackexchange_scraper.py   API Stack Exchange (~500k items)
│   ├── proofwiki_scraper.py       Web scraping ProofWiki (~20k)
│   ├── arxiv_scraper.py           Sources LaTeX arXiv (~100k)
│   └── french_courses_scraper.py  Exo7, Bibmath, etc. (~50k)
│
├── 🔧 UTILITAIRES (utils/)
│   ├── __init__.py
│   ├── cleaner.py               Nettoyage et validation données
│   └── storage.py               Stockage + anti-doublons
│
└── 📦 CONFIGURATION
    ├── requirements.txt         Dépendances Python
    └── .gitignore              Fichiers à ignorer (Git)


WORKFLOW D'UTILISATION
=======================

1. INSTALLATION
   ./install.sh                  (Linux/Mac)
   ou suivre instructions Windows dans START_HERE.md

2. TEST RAPIDE
   python test.py                Vérifier que tout fonctionne

3. PREMIER SCRAPING
   python main.py                Collecter ~2k items (5 min)

4. ANALYSE
   python analyze.py             Voir statistiques

5. PRODUCTION
   python production_scraping.py sample      # 5k items
   python production_scraping.py production  # 600k+ items


DONNÉES GÉNÉRÉES
=================

math_dataset/                  (créé après scraping)
├── raw/                      Données brutes par source
│   ├── stackexchange/
│   │   ├── batch_YYYYMMDD_HHMMSS.json
│   │   └── ...
│   ├── proofwiki/
│   ├── arxiv/
│   └── french_courses/
│
├── processed/                Prêt pour ML
│   ├── train.jsonl          80% (entraînement)
│   ├── validation.jsonl     10% (validation)
│   └── test.jsonl           10% (test)
│
├── index.json               Index anti-doublons
└── scraping_stats.json      Statistiques de collecte


FORMAT DES DONNÉES
===================

Chaque item contient:
- id              : Identifiant unique
- source          : stackexchange|proofwiki|arxiv|french
- question/answer : Pour exercices
- theorem/proof   : Pour preuves formelles
- tags            : Catégories mathématiques
- language        : fr|en
- proof_structure : Type (induction, absurde, etc.)
- metadata        : Infos additionnelles

Exemple:
{
  "id": "se_12345",
  "source": "stackexchange",
  "question": "Montrer que pour tout n ∈ ℕ...",
  "answer": "Par récurrence...",
  "proof_structure": {
    "type": "induction",
    "techniques": ["mathematical_induction"],
    "steps": ["base_case", "inductive_step"]
  },
  "tags": ["induction", "number-theory"],
  "language": "fr",
  "score": 42
}


VOLUMÉTRIE ATTENDUE
====================

Source              Items      Temps    Qualité
----------------------------------------------
Stack Exchange      500,000    2-3h     ⭐⭐⭐⭐
ProofWiki           20,000     1h       ⭐⭐⭐⭐⭐
arXiv               100,000    10-15h   ⭐⭐⭐⭐
Cours français      50,000     2-3h     ⭐⭐⭐⭐
----------------------------------------------
TOTAL               670,000+   16-24h


CARACTÉRISTIQUES DU SYSTÈME
=============================

✅ Production-ready
   - Gestion erreurs robuste
   - Rate limiting respecté
   - Sauvegarde incrémentale
   - Peut être interrompu/repris

✅ Scalable
   - Scrapers parallèles (asyncio)
   - Anti-doublons automatique
   - Pas de limite de taille

✅ Qualité
   - Filtres validation multiples
   - Détection langue (FR/EN)
   - Extraction structure preuve
   - Nettoyage HTML/LaTeX

✅ Flexible
   - Facile d'ajouter sources
   - Configuration personnalisable
   - Multiples formats export


PROCHAINES ÉTAPES APRÈS SCRAPING
==================================

1. Exploration des données
   python analyze.py

2. Préparation pour Lean
   - Parser structure mathématique
   - Créer templates conversion
   - Normaliser notations

3. Fine-tuning modèle (avec 4×A100)
   - Fine-tune DeepSeek-Prover (7B)
   - Ou LLEMMA
   - LoRA/QLoRA pour optimiser mémoire
   - Dataset: math_dataset/processed/train.jsonl

4. Validation
   - Utiliser compilateur Lean
   - Vérifier outputs générés
   - Itérer sur qualité


LIENS UTILES
=============

Stack Exchange API:
  https://api.stackexchange.com/docs

ProofWiki:
  https://proofwiki.org

arXiv bulk data:
  https://arxiv.org/help/bulk_data

Lean documentation:
  https://leanprover.github.io

DeepSeek-Prover:
  https://github.com/deepseek-ai/DeepSeek-Prover


SUPPORT & DÉPANNAGE
====================

Logs détaillés: scraping.log

Problèmes courants:
- Rate limit exceeded → Attendre ou clé API
- Scraping lent → Normal pour arXiv
- Espace disque → ~3 GB pour production

Tous les tests passent? → Prêt à l'emploi! 🎉
